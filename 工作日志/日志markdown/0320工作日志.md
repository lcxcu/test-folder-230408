###  实验设计
1. **变量的选择：** 季度变量（源于财报：inventory；accounts receivable；capital expenditures;gross margin;SG&A）；月度变量（CRSP计算所得:Abnormal stock return; monthly Return volatility）；日度变量（源于CRSP: daily Return volatility) ; 并选择有[[49个连续季度可用数据]]的企业进行实验
2. **构建基于深度学习的混合频率回归(deep learning&OLS)模型:**
==论证模型在预测上具有**结构稳定性**与**关系稳定性**==
- 1. **混频变量的频率处理**：以最低频率变量为基准，将月度变量与日度变量分别取同时间区间的数据；
	- 例如：1个A季度频率的数据，同时加上A季度时间段里的3个月度频率的数据，再加上A季度时间段里的28×3个日度数据
- 2. **预测方法：** 
	-  **预测周期**：滚动预测选用[[40个季度的滚动窗口]]对预测t1年；t2年;  t3年的eps(季度的每股收益)进行预测
	- **预测变量**：
		- **频率的重要性比较**： 控制变量法（季度/月度/日度）依次从变量中删去，进行预测，比较三者样本外预测的预测损失值
		- **变量的重要性比较：** 控制变量法（14个变量在预测短中长周期时的loss值)
*可以得出下面这样的表格：*

|\\ |del_var1|del_car2|...|del_var14|
|--|--|--|--|--|
|t1|loss1_1|loss1_2|...|loss1_14|
|t2|loss2_1|loss2_2|...|loss2_14|
|t3|loss3_1|loss3_2|...|loss3_14|

|\\ |del_month|del_quarter|del_daily|
|--|--|--|--|
|t1|loss1_m|loss1_q|loss1_d|
|t2|loss2_m|loss2_q|loss2_d|
|t3|loss3_m|loss3_q|loss3_d|

- 3. **模型性能比较(在预测周期不同时的比较)**：
- 分析师共识预测
- 组合的混合频率模型
- （PW/HDZ/AR)

---
**代码：** 日度数据真的很大,用矩池云读+处理也很久，在进行日度+月/季度表的连接
```python
# 分批读取df_crsp
df_crsp_list = []
df_crsp_temp = pd.read_csv('/mnt/Data_US/crsp/CRSP_csv/CRSP_csv.csv',nrows = 1000000,usecols = ['Ticker','CUSIP','YYYYMMDD','DlyRet'])
df_crsp_list.append(df_crsp_temp)
for i in range(1,18):
    print(i)
    rows = 1000000*i
    df_crsp_temp = pd.read_csv("/mnt/Data_US/crsp/CRSP_csv/CRSP_csv.csv",usecols=['Ticker','CUSIP','YYYYMMDD','DlyRet']
                  ,skiprows = [i for i in range(1,rows)]
                  ,nrows=1000000,low_memory=False)
    df_crsp_list.append(df_crsp_temp)
df_crsp = pd.concat(df_crsp_list)
df_crsp = df_crsp.drop_duplicates()


def daliy_to_quarter(df,var,id_code):
    # df是变量为日度数据的dataframe；var_cols是预测变量列名,id_code是自选择的标识符
    code_dfs = []
    df_group = df.groupby(id_code)
    for code,code_df in df_group :
        lag_n = 90 # 日度->季度度 频率*3
        for i in range(0,lag_n):
            code_df[var+'_d'+str(i+1)] = code_df[var].shift(-i)
        code_df = code_df.dropna()
        code_dfs.append(code_df)
    df = pd.concat(code_dfs)
    df = df.drop(columns = var)
#     df = df.dropna()
    df = df.drop_duplicates(['Ticker','fyearq','fqtr'])
    return df

df_var14q = daliy_to_quarter(df_var14.iloc[:10000,:],'var14','Ticker')
df_q.merge(df_var14q,on = ['Ticker','fyearq','fqtr'],how = 'left')
```