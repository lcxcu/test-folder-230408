**Table of Contents**
-  [[#局部解释模型|局部解释模型]]
- [[#Additive feature attribution methods|Additive feature attribution methods]]
-  [[#LIME|LIME]]
-  [[#Kernel  SHAP|Kernel  SHAP]]
	-  [[#Kernel  SHAP#举例|举例]]
	- [[#Kernel  SHAP#SHAP 与 LIME|SHAP 与 LIME]]
	- [[#Kernel  SHAP#Kernel SHAP公式说明|Kernel SHAP公式说明]]
-  [[#XAI method 选择原因|XAI method 选择原因]]
	- [[#XAI method 选择原因#SHAP|SHAP]]
-  [[#批注格式|批注格式]]
	-  [[#批注格式#1 无插件版本，使用obsidian默认批注|1 无插件版本，使用obsidian默认批注]]
	-  [[#批注格式#2 有插件版本，使用Comments插件|2 有插件版本，使用Comments插件]]

---
### 小总结
- [ ]  **KernelExplainer**的代码实现还没有尝试：  [Explain Your Machine Learning Predictions With Kernel SHAP (Kernel Explainer) | by Chetan Ambi | Towards AI](https://medium.com/towards-artificial-intelligence/explain-your-machine-learning-predictions-with-kernel-shap-kernel-explainer-fed56b9250b8)
- [ ] 已收集XAI论文内容，均来自SCI-E，可从中总结各XAI的选择原因与优缺点:
- [[Explainable artificial intelligence_an analytical review.pdf]]
- [[A survey on neural network interpretability.pdf]]
- [[Explainable AI_A Review of Machine Learning.pdf]]

---
#### Shapley kernel
>Shapley kernel的原始论文：[A Unified Approach to Interpreting Model Predictions(neurips.cc)](https://proceedings.neurips.cc/paper/2017/file/8a20a8621978632d76c43dfd28b67767-Paper.pdf)，论文侧重于**局部方法**，即针对**单个输入变量x解释预测结果**的方法。

---
###### 局部解释模型
- 解释模型通常使用简化后的输入x'，通过映射函数$x = h_x(x')$映射到原始输入。
- **局部方法**试图确保当z' ≈ x'时，$g(z') ≈ f(h_x(z'))$。尽管$x'$可能包含比$x$更少的信息，但$h_x$是特定于当前输入x的，因此$h_x(x')=x$。
	-  简化的输入  $x'$ ：对输入的复杂特征进行解释时，只考虑每个特征对输出的贡献，而忽略其他特征之间的交互作用
	- 例如，对于文本数据，可以使用词袋模型(Bag-of-word)将文本转换为向量表示，然后将这些向量转换为二进制表示（例如，1表示单词出现在文本中，0表示单词未出现）。
	- 对于图像数据，可以使用超像素分割算法将图像分成多个块，并将每个块表示为二进制值（例如，1表示块存在，0表示不存在）。
	- 在这些示例中，简化的输入特征是二进制的，数量较少，比原始输入数据更容易解释和理解。
>下面公式中，f代表**需要解释的模型**，g代表**模型的解释模型**

---
##### Additive feature attribution methods
- **加性特征归因方法**是下面构建解释模型的基础：
$$
g(z') = \phi_0 + \sum_{i=1}^{M} \phi_i z_i'\quad \text{(1)}
$$
- 其中，$z' ∈ \{0, 1\}^M$，M是简化的**输入特征的数量**
	- 即每个输入特征都用$\{0,1\}$表示
	- $\phi_i ∈ R$ ，表示**每个特征对预测结果的影响程度**
与公式(1)定义相匹配的解释模型的方法将**每个特征的影响程度$\phi_i$归因于每个特征**，而**将所有特征归因的效应求和可以近似原始模型的输出$f(x)$**
- 这个公式的意义是，M个特征，每个特征对预测结果产生的影响是$\phi_i$ ,$\phi_0$则类似于残差项的影响程度，是由于M个特征的综合影响而导致模型最终预测结果。
---
##### LIME
LIME是加性的特征归因方法之一，该方法使用的**局部线性解释模型**完全遵循公式(1)。
- LIME将简化的输入$x'$称为“可解释输入”，而**映射$x=h_x(x')$将可解释输入的二进制向量转换为原始输入空间**。
- **不同类型的$h_x$映射用于不同的输入空间**。
	- 例如，对于词袋文本特征，$h_x$将1或0（存在或不存在）的向量转换为原始词数，如果简化输入为1，则为1，如果简化输入为0，则为0。
	- 对于图像，$h_x$将图像视为一组超像素，将1映射为保留超像素的原始值，将0映射为用相邻像素的平均值替换超像素（这意味着它被视为缺失值）。

LIME使用以下目标函数计算特征影响程度$\phi$:
$$
ξ = \underset{g \in G}{arg{}\ min} L(f, g, π_{x'} ) + Ω(g) \quad \text{(2)}
$$
- 其中，$L$是**针对简化输入空间中*一组样本*的损失函数**。
	- 针对一组简化后的样本值，通过最小化实际预测值与解释模型预测值之间的差异的平方和，得到每一组样本的损失函数L。
- 局部核$π_x'$对损失函数计算加权，为简化输入空间的每个样本分配权重，来确保解释模型$g(z')$对于原始模型$f(h_x(z'))$的准确性。
- $Ω$是对解释模型$g$复杂度的惩罚项。
- 由于在LIME中解释模型$g$遵循公式1并且$L$是平方损失，因此可以使用惩罚线性回归来计算公式(2)（: 特征影响程度）

- 为了计算公式(2)，我们首先可以将$g(z′)$表示为基函数$\phi(z′)$的线性组合:
  - $$
g(z') = \sum_{j=1}^{M}\beta_j\phi_j(z')
$$
  - 其中$\beta_j$是回归系数，$\phi_j(z')$是基函数。然后我们可以最小化以下目标函数:
  -  

 $$
	\hat{\beta} = \underset{\beta}{arg\ min}\left[ \sum_{i=1}^n \left(y_i-\sum_{j=1}^M \beta_j \phi_j(z'_i)^2 + \lambda \sum_{j=1}^M |\beta_j| \right) \right]
$$
  - 其中$y_i = f(x_i)$是原始模型对第$i$个实例的预测，$\lambda$是控制惩罚项$\sum_{j=1}^M |\beta_j|$强度的正则化参数。惩罚项用于防止过拟合，并使得归系数尽可能小。
  - 结果系数$\hat{\beta}$然后可以使用以下公式来计算特征重要性程度（特征对目标值的影响程度）:
  - $$
\phi_j(z') = {1\over{n}}\sum_{i=1}^n \hat{\beta}_j\phi_j(z'_i)
$$ 
-  在公式2中，$L$是针对简化输入空间中一组样本的损失函数，而$\hat{\beta}$是使用惩罚线性回归计算出来的权重系数。
   - 公式2的目标是最小化$g$的复杂度$Ω(g)$和损失$L$。
   - 因此，我们可以将$\hat{\beta}$视为特征对目标值的影响程度的权重，并使用公式来计算每个特征的重要性$\phi_j$。 
   - 具体而言，$\hat{\beta}_j$表示特征$j$的权重系数，$\phi_j(z'_i)$是样本$i$在生成简化样本$x'$时，对特征$j$的影响程度。 所以，$\hat{\beta}_j\phi_j(z'_i)$表示样本$i$对特征$j$的重要性程度，然后我们取所有样本重要性的平均值，即可计算出所有样本中特征$j$的平均重要性程度$\phi_j$。
   - 其中$\phi_j$是特征$j$的重要性分数，$\phi_j(z'_i)$为$i$实例的$j$基函数的值

总结一下LIME的实现：
1.  将简化输入$x'$转换为原始输入$x=h_x(x')$。
2.  对于给定的样本，计算在$x'$上的局部权重核$π_{x'}$。
3.  使用加权的线性回归模型来拟合局部模型$g$，使得在给定权重核下的损失函数$L(f,g,π_{x'})$最小化。
4.  添加一个正则化项$Ω(g)$来惩罚复杂度。
5.  使用交叉验证或其他方法来选择最优的正则化参数。
6.  最后得到的局部模型$g$的输出即为$\phi(x)$，即特征$x$的重要性分数。

---
#### Kernel  SHAP
Kernel  SHAP 是一种与模型无关的方法，**可用于估计任何模型的 SHAP 值**。因为它不对模型类型做出假设，所以 KernelExplainer 也会比其他特定于模型类型的算法慢。
- Kernel SHAP 技术是基于 LIME 技术的一种改进版本，用于解释机器学习模型的预测结果。
- 在 Kernel SHAP 中，**使用加权线性回归作为局部代理模型**，并使用一种适当的加权核函数来分配新生成的样本的权重。然后，**使用这个局部代理模型来解释机器学习模型的预测结果**。

在 SHAP 论文中，作者们证明了，在**使用加权线性回归模型和适当的权重核函数**时，局部代理模型的**回归系数可以估计出 SHAP 值**。SHAP 值是一种用于解释机器学习模型的决策的技术，它可以计算每个特征对于模型输出的贡献度。
>昨天笔记中的Shapley的内容，只看公式不好理解，但是学习Kernel SHAP之前还是简单回顾一下
![[0331工作日志#Shapley]]

>参考链接：[9.6 SHAP (SHapley Additive exPlanations) | Interpretable Machine Learning](https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap)

Kernel SHAP 估计（预测模型的某次预测结果x） 实例x 中 每个特征值对预测的贡献。 
Kernel SHAP 由五个步骤组成：
-   联盟样本$z'_k \in \{0,1\},k\in \{1,...,K\}$（1 = 联盟中存在的特征，0 = 不存在特征）。
-   获取每个的预测$z_k'$,通过首先转换$z_k'$到原始特征空间，然后应用模型$\hat{f}:\hat{f}(h_x(z_k'))$
-   计算每个的权重$z_k'$与 Kernel SHAP。
-   拟合加权线性模型。
-   返回Kernel SHAP值$\phi_k$，即线性模型中的系数

---
##### 举例
- 我们可以通过反复抛硬币来创建一个随机联盟，直到我们有一个 0 和 1 的链。 例如，（1，0，1，0） 的向量表示我们有第一个和第三个特征的联盟。 K 采样联盟将成为回归模型的数据集。 回归模型的目标是对联盟的预测。 
- 为了从特征值联盟转换到有效数据实例，我们需要一个函数
$$
h_x(z') = z\ 
使得 h_x:\{0,1\}^M \to R^p
$$

- 函数$h_x$将 1 映射到我们要解释的实例 X 中的对应特征值。
- 对于表格数据，它将 0 映射到我们从数据中采样的另一个实例的值（非此实例X的对应特征值）。 这意味着我们将“*特征值不存在*”等同于“*特征值被数据中的随机特征值替换*”。 对于表格数据，下图可视化了从联盟到特征值的映射：
![](https://christophm.github.io/interpretable-ml-book/images/shap-simplified-features.jpg)


$h_x$对于表格数据处理时，**其中特征$x_j$和$x_{-j}$（其他特征）被视为相互独立**，并在边缘分布上进行积分：
$$
\hat{f(h_x(z'))} = E_{X_{-j}}[\hat f(x)]
$$
从边际分布中抽样意味着忽略当前特征和不存在特征之间的依赖关系结构。

 因此，Kernel SHAP 与所有基于排列的解释方法都存在相同的问题。 该估计对不太可能的情况给予了过多的重视。 结果可能变得不可靠。 
 - 但需要从边际分布中抽样。
原因是从条件分布中采样时，虽然从条件分布中采样可以避免这个问题，但这会改变值函数，从而改变Shapley值作为解决方案的游戏，导致Shapley值的解释方式发生变化。例如，当使用条件抽样时，可能根本没有使用的特征可以具有非零的Shapley值。

例如对于图像，下图描述了可能发生的条件预测时映射函数：
- 对于图像处理，缺少特征也就是 （0）时，$h(x)$的相应区域应该变灰
- 但明显样本中没有办法使sp3为0的情况出现
![](https://christophm.github.io/interpretable-ml-book/images/shap-superpixel.jpg)
---
##### SHAP 与 LIME
**与 LIME 的最大区别在于回归模型中实例的权重。**
- LIME 根据实例与原始实例的接近程度（最小损失函数）对实例进行加权。 联盟向量中的 0 越多，LIM 中的权重越小。
- SHAP 方法中采用的权重是**基于 Shapley 值估计**得到的权重。（ *根据这个特征的丢失或存在对预测结果的造成的影响* 而进行的加权）
	 - 对于一个特征的重要性程度，*与它所在的组合的大小有关*。
	 - 组合中1的数量***越少或越多，该组合对应的特征的重要性就越高***。
	 - 这个性质的直觉解释是，我们***最容易了解一个特征的影响***是在它在其他特征影响下被隔离出来研究，也就是***当一个特征单独存在或者与其他特征的影响完全不同的时候***。
		 - 如果一个组合只有一个特征，那么这个特征的影响就可以被单独考虑；如果一个组合除了某一个特征以外其他特征都有时，那么这个特征的总影响（包括主效应和特征交互）就可以较容易的被研究。
		 - 而如果一个组合中包含了一半的特征，那么对于某个特定的特征，该特定特征对该特征组合的影响与其他许多包含该特征的组合的影响是相同的，因此对于该特征的贡献的了解程度就比较有限。

---
##### Kernel SHAP公式说明
为了实现与Shapley兼容的权重，Lundberg等人提出了Kernel SHAP：
在 Kernel SHAP 中，使用的权重核函数被称为 Shapley 核函数，它可以恢复出 Shapley的值。Shapley 核函数的计算公式如下：
$$
\pi_{x'}(z') = \frac{(M-1)}{{M \choose |z'|}|z'|(M-|z'|)} ,
$$
其中，$M$ 是最大联盟规模（特征组合中特征数量），$|z'|$ 是样本 $z'$ 中特征的数量。这个公式的含义是，对于给定的特征样本(特征子集) $z'$，计算**它对应的 Shapley 值的权重**。
- $\pi_{x'}(z')$ 表示将子集 $z'$ 与其余特征的排列组合进行比较时，子集$z'$所占权重
	- 分母表示了选出 $|z'|$ 个样本的方案数，而分子表示了每个样本被选中的可能性，因此二者的比值可以用来对样本进行加权，从而计算 Shapley 值
	- 分母中的 $|z'|(M-|z'|)$ 表示了一个样本被选中后，还需要选中剩下的 $(M-|z'|)$ 个样本，因此乘上这两个值就可以得到选出 $|z'|$ 个样本的总方案数。而分子中的 $(M-1)$ 则表示在这些方案中，每个样本都被赋予相同的权重，即每个样本都有相同的可能性成为最终选中的样本之一。

因此，Kernel SHAP 可以使用局部代理模型来解释机器学习模型的预测结果，并计算每个特征对于模型预测的贡献度，从而提供对模型决策的解释和洞察构建加权线性回归模型：
$$
g(z') = \phi_0+\sum_{j=1}^M \phi_j z_j'
$$
通过优化以下损失函数 L 来训练线性模型 g：
$$
L(\hat{f},g,\pi_x) = \sum_{z' \in Z}[\hat{f(h_x(z'))-g(z')}]^2 \pi_x(z')
$$

其中 Z 是训练数据。 这是我们通常针对线性模型优化的损失函数平方和。 
模型的估计系数$\phi_j$是shapley值。

---
#### XAI method 选择原因
对于简单的模型而言，最好的解释是模型本身，因为它可以完美地表达自己，易于理解。但对于复杂的模型，例如集成方法或深度网络，我们不能使用原始模型作为其自己的最佳解释，因为它不容易理解。因此，我们必须使用一个更简单的解释模型，即任何可解释原模型的近似模型。

		The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. Instead, we must use a simpler explanation model, which we define as any interpretable approximation of the original model. 

##### SHAP
- [[Explainable AI_A Review of Machine Learning.pdf|Explainable AI_A Review of Machine Learning]]


1. 在所介绍的方法中，**SHAP是最完整的方法**，它可以在全局和局部范围内为任何模型和任何类型的数据提供解释。**但SHAP并不是没有缺点:** SHAP的核版本，像大多数基于排列的方法一样，KernelSHAP没有**考虑到特征依赖**，因此**往往会给不太可能的数据点分配过高的权重，导致解释结果不准确**，而SHAP的树版本**TreeSHAP**则解决了这个问题，因为它**考虑了特征之间的依赖关系**。但由于TreeSHAP依赖于条件预测值，因此可能会**导致一些没有对预测结果产生影响的特征被分配一个非零的重要性值**，这可能会导致解释结果不直观。

		Of the methods presented, SHAP is the most complete method, providing explanations for any model and any type of data, doing so at both a global and local scope. However, SHAP is not without shortcomings: The kernel version of SHAP, KernelSHAP, like most permutation based methods, does not take feature dependence into account, thus often over-weighing unlikely data points and, while TreeSHAP, the tree version of SHAP, solves this problem, its reliance on conditional expected predictions is known to produce non-intuitive feature importance values as features with no impact on predictions can be assigned an importance value that is different to zero.
		
2. 在解释任何黑盒模型方面, **LIME 和 SHAP 方法是目前文献中最全面、最主流的可视化特征交互和特征重要性的方法**，而 Friedman 的 PDP 方法虽然更为古老、不够复杂，但仍然是一个流行的选择。**LIME 和 SHAP 方法不仅与模型无关，而且已被证明适用于任何类型的数据。换句话说，它们能够解释黑盒模型的输出，并将其归因于输入特征的重要性。** LIME 和 SHAP 方法是两种主要不同的方法，LIME 建立了一个解释模型，该模型是可解释的模型，而 SHAP 是一种基于 Shapley 值的方法，该方法能够将预测输出分解为每个输入特征的贡献。

		In terms of explaining any black-box model, the LIME [45] and SHAP [48] methods are, by far, the most comprehensive and dominant across the literature methods for visualising feature interactions and feature importance, while Friedman’s PDPs [59], although much older and not as sophisticated, still remains a popular choice. The LIME and SHAP methods are not only model-agnostic, but they have been demonstrated to be applicable to any type of data.
-----
#### 批注格式
##### 1 无插件版本，使用obsidian默认批注
obsidian默认快捷键`ctrl+/`,会生成下面这样的批注，后面用 #comment 形成标签方便查找：
**批注**在阅读视图下不可见，编辑模式下可见，刚好不影响笔记整体观感
**标签**会高亮显示，即使阅读视图下也可以看到这里有标签，可提醒读者在阅读试图下这里有批注

%%这段话我在这里插入部分批注%% #comment 

```
%%这段话我在这里插入部分批注%% #comment
```
##### 2 有插件版本，使用Comments插件
安装插件：[Comments](obsidian://show-plugin?id=obsidian-comments)
我的markdown文件夹中已经安装了，可以直接使用。

**使用方法：**
1. 确认安装了[Comments](obsidian://show-plugin?id=obsidian-comments)插件，并在*设置*中已经启用插件
2. 设置快捷键快速插入批注：
	 在`设置`中找到`快捷键`选项，搜索`Comments:Add Comment`设置**评论**快捷键,我的笔记中设置为`ctrl+Q`
显示结果如下：
**选中**下面这段文字，按住`ctrl+Q`
（**一定要选中**文字段落再按**ctrl+Q**）
![[Comments_example.gif]]

笔记内容在这里，这里我写了笔记，这里是对我的笔记进行评论的内容

现在，我按住ctrl+E，切换到阅读视图，展示评论内容

<label class="ob-comment" title="" style=""> 假设这里是我的笔记内容，现在我已经安装Comennts插件，现在可以对这段话插入评论 <input type="checkbox"> <span style="">这里是我对这段话的评论，鼠标点击会显示我的评论内容，非常方便 </span></label>

<label class="ob-comment" title="" style=""> 再展示一次
 <input type="checkbox"> <span style=""> <label class="ob-comment" title="" style=""> 评论内容 <input type="checkbox"> <span style=""> 甚至可以同样选中评论内容，对评论进行评论</span></label></span></label>

